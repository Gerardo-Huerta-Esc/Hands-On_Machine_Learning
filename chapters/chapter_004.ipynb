{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 - Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Palabras para este capítulo:\n",
    "tweaks - ajustes\n",
    "several - varias/varios\n",
    "still - aún\n",
    "switching  - transpuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudiaremos la regresión lineal. Uno de los algoritmos más simples. Se abordará de dos métodos de entrenamientos muy distintos:\n",
    "- Utilizando una ecuación cerrada (closet-form equation) la cual calcula los parámetros del modelo que más se ajustan a los datos de entrenamiento (es decir, los parámetros del modelo que minimizan la función de costo sobre el conjunto de entrenamiento)\n",
    "- Usando el Descenso del Gradiente (Gradient Descent \"GD\") el cual ajusta gradualmente los parámetros del modelo para minimizar la función de costo sobre el conjunto de entrenamiento. Eventualmente este método converge al mismo conjunto de parámetros que el primer método.\n",
    "\n",
    "Después revisaremos la Regresión Polinómica, un modelo más complejo que puede abordar datos no lineales. Ya que este modelo tiene más parámetros que el modelo de Regresión Lineal, es más propenso a sobre ajustarse a los datos de entrenamiento. Por lo que estaremos aprendiendo a detectar si este es el caso o no, usando curvas de aprendizaje (learning curves), y después veremos varias técnicas de regularización que pueden reducir el riezgo de sobre ajuste en los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manera general, un modelo lineal hace una predicción calculando la suma ponderada de las características de entreda, mas una constante llamada término de sesgo (o término de intersección)\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n$$\n",
    "\n",
    "- $\\hat{y}$ es el valor predicho.\n",
    "\n",
    "- $n$ es el número de características.\n",
    "\n",
    "- $x_i$ es el valor de la característica $i^{th}$.\n",
    "\n",
    "- $\\theta_j$ es el parémetro del modelo  $j^{th}$ (incluido el término de sesgo $\\theta_0$ y la característica de pesos $\\theta_1 + \\theta_2 + ,..., + \\theta_n$)\n",
    "\n",
    "Esto puede escribirse de una forma más concisa usando la notación vectorial:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{y} = h_{\\theta}(x) = \\mathbf{\\theta} \\cdot \\mathbf{x}\n",
    "$$\n",
    "(no se aprecia bien, pero $\\theta$ está en negrita porque es un vector, igual que $\\mathbf{x}$)\n",
    "\n",
    "En esta ecuación:\n",
    "- $\\mathbf{\\theta}$ es el vector de parámetros del modelo, que contiene el término de sesgo $\\theta_0$ y los pesos de las características $\\theta_1$ a $\\theta_n$.\n",
    "- $\\mathbf{x}$ es el vector de características de la instancia, que contiene $x_0$ a $x_n$, con $x_0$ siempre igual a 1.\n",
    "- $\\mathbf{\\theta} \\cdot \\mathbf{x}$ es el producto punto de los vectores $\\theta$ y $\\mathbf{x}$, que por supuesto es igual a $\\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_n x_n$.\n",
    "- $h_{\\mathbf{\\theta}}$ es la función de hipótesis, usando los parámetros del modelo $\\theta$.\n",
    "\n",
    "En machine learning, una función de hipótesis es una función matemática que representa el modelo que estás utilizando para hacer predicciones, es fundamental porque dedine cómo el modelo transforma las entradas en predicciones. En otras palabras, la función de hipótesis $ h_{\\theta}(x)$ utiliza los parámetros $\\theta$ y los valores de las características $\\mathbf{x}$ para calcular la predicción de la salida.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
